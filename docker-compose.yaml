services:
  tta:
    hostname: tta
    extra_hosts:
      - "tta:127.0.0.1"
    image: ghcr.io/vuno/tta:${IMAGE_NAME:-tta}
    ipc: host  
    network_mode: host  
    tty: true
    init: true
    stdin_open: true
    volumes:  
      - .:${PROJECT_ROOT:-/opt/lct} 
    working_dir: ${PROJECT_ROOT:-/opt/lct}
    user: ${UID}:${GID}
    environment: 
      TZ: ${TZ:-Asia/Seoul}
      CUDA_DEVICE_ORDER: PCI_BUS_ID
    build: 
      target: ${TARGET_STAGE:-train}
      context: .
      dockerfile: Dockerfile
      args: 
        LINUX_DISTRO: ${LINUX_DISTRO:-ubuntu}
        DISTRO_VERSION: ${DISTRO_VERSION:-20.04}
        CUDA_VERSION: ${CUDA_VERSION:-11.2.2}
        CUDNN_VERSION: ${CUDNN_VERSION:-8}
        PYTHON_VERSION: ${PYTHON_VERSION:-3.10}
        PROJECT_ROOT: ${PROJECT_ROOT:-/opt/lct}
        PYTORCH_VERSION: ${PYTORCH_VERSION:-1.13.1}
        TORCHVISION_VERSION: ${TORCHVISION_VERSION:-0.14.1}
        PYTORCH_HOST: ${PYTORCH_HOST:-download.pytorch.org}
        PYTORCH_INDEX_URL: ${PYTORCH_INDEX_URL:-https://download.pytorch.org/whl/cu117}
        CONDA_URL: ${CONDA_URL:-https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh}
        # Optional mirror links for faster `apt` package installation.
#        DEB_OLD: ${DEB_OLD:-http://archive.ubuntu.com}
#        DEB_NEW: ${DEB_NEW:-http://mirror.kakao.com}
        GRP: ${GRP}
        USR: ${USR}
        GID: ${GID}
        UID: ${UID}
        TZ: ${TZ:-Asia/Seoul}
        PILLOW_SIMD_VERSION: ${PILLOW_SIMD_VERSION:-9.0.0.post1}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]

  deploy:
    hostname: lct
    image: ghcr.io/vuno/lct-deploy:${IMAGE_NAME:-lct}
    tty: true
    init: true
    stdin_open: true
    volumes: 
      - .:${PROJECT_ROOT:-/opt/lct} 
    working_dir: ${PROJECT_ROOT:-/opt/lct}
    environment: 
      CUDA_DEVICE_ORDER: PCI_BUS_ID
      CUBLAS_WORKSPACE_CONFIG: ':4096:8'

    build: 
      target: ${TARGET_STAGE:-deploy}
      context: .
      dockerfile: Dockerfile
      args: 
        LINUX_DISTRO: ${LINUX_DISTRO:-ubuntu}
        DISTRO_VERSION: ${DISTRO_VERSION:-20.04}
        CUDA_VERSION: ${CUDA_VERSION:-11.2.2}
        CUDNN_VERSION: ${CUDNN_VERSION:-8}
        PYTHON_VERSION: ${PYTHON_VERSION:-3.10}
        PROJECT_ROOT: ${PROJECT_ROOT:-/opt/lct}
        PYTORCH_VERSION: ${PYTORCH_VERSION:-1.13.1}
        TORCHVISION_VERSION: ${TORCHVISION_VERSION:-0.14.1}
        PYTORCH_HOST: ${PYTORCH_HOST:-download.pytorch.org}
        PYTORCH_INDEX_URL: ${PYTORCH_INDEX_URL:-https://download.pytorch.org/whl/cu117}
        CONDA_URL: ${CONDA_URL:-https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]

  label:
    hostname: lct
    image: projectmonai/monailabel:latest
    ipc: host
    tty: true
    init: true
    stdin_open: true
    network_mode: host
    volumes:
      - .:/workspace
    environment:
      TZ: ${TZ:-Asia/Seoul}
      CUDA_DEVICE_ORDER: PCI_BUS_ID
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
